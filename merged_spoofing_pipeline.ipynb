{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yusbad09/Autonomous-Vehicle-GPS-Spoofing-detection/blob/main/merged_spoofing_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fePt2G0Ir_4S"
      },
      "source": [
        "# Unified GPS & IMU Spoofing Detection Pipeline\n",
        "\n",
        "**Author:** Badrudeen Yusuf Akinkunmi (adapted)\n",
        "**Date:** 2025-08-11\n",
        "\n",
        "This notebook merges two previously separate evaluation pipelines into a single reproducible research-style experiment:\n",
        "\n",
        "- **A:** 80:20 train/test split evaluation (RandomForest, XGBoost) + optional LSTM Autoencoder (thresholded labels)\n",
        "- **B:** Stratified K-Fold cross-validation (k=5) evaluation for RandomForest & XGBoost (averaged metrics). Only the **last fold's** visualizations are included in the final PDF (per instruction).\n",
        "- **C:** LSTM Autoencoder for unsupervised detection (trained on clean data, threshold = 95th percentile on clean errors) — thresholded labels used to compute comparable metrics.\n",
        "- **D:** A single consolidated PDF report with both Split and CV comparisons, AE results, and plots.\n",
        "\n",
        "All sections are heavily documented. Run cells top-to-bottom. Plots are shown inline and also saved to `outputs_combined/`."
      ],
      "id": "fePt2G0Ir_4S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sNSCX2mr_4V"
      },
      "source": [
        "## 0. Install dependencies (run if needed)\n",
        "\n",
        "Uncomment the `!pip` lines if your environment doesn't already have these packages installed (Colab, fresh VM, etc.)."
      ],
      "id": "6sNSCX2mr_4V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3zWb1ozr_4W",
        "outputId": "01fe0739-f980-4a47-8b72-9e0368d095e2"
      },
      "source": [
        "# Uncomment to install packages in a fresh environment\n",
        "!pip install reportlab xgboost torch --quiet\n",
        "\n",
        "print('Skip install step if packages already present')"
      ],
      "id": "L3zWb1ozr_4W",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSkip install step if packages already present\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhurS8ptr_4Y"
      },
      "source": [
        "## 1. Imports\n",
        "Standard scientific stack, ML models, PyTorch for AE, and ReportLab for PDF generation."
      ],
      "id": "jhurS8ptr_4Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xcvnor5nr_4Z",
        "outputId": "cd597a64-cf38-45a9-a02a-e2a6034aae1f"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (confusion_matrix, roc_auc_score, roc_curve,\n",
        "                             precision_recall_fscore_support, accuracy_score)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8,4)\n",
        "print('Imports OK')"
      ],
      "id": "Xcvnor5nr_4Z",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvRxXy_3r_4a"
      },
      "source": [
        "## 2. Configuration & Utility helpers\n",
        "Set seeds for reproducibility and directories for outputs. `ensure_dir` makes sure directories exist."
      ],
      "id": "wvRxXy_3r_4a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WeAMeWyr_4a",
        "outputId": "0eebf1ac-6668-4f29-95df-ed0303b183b7"
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "OUT_ROOT = 'outputs_combined'\n",
        "FIGS_DIR = os.path.join(OUT_ROOT, 'figures')\n",
        "AE_DIR = os.path.join(OUT_ROOT, 'autoencoder')\n",
        "N_SPLITS = 5\n",
        "DO_AE = True\n",
        "AE_SEQ_LEN = 20\n",
        "AE_EPOCHS = 15\n",
        "AE_BATCH = 32\n",
        "\n",
        "def ensure_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "ensure_dir(OUT_ROOT); ensure_dir(FIGS_DIR); ensure_dir(AE_DIR)\n",
        "print('Configuration set. Outputs ->', OUT_ROOT)"
      ],
      "id": "8WeAMeWyr_4a",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set. Outputs -> outputs_combined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ABNSEir_4b"
      },
      "source": [
        "## 3. Synthetic GNSS + IMU dataset generator\n",
        "Generates a plausible random-walk trajectory and injects spoof windows (abrupt or gradual)."
      ],
      "id": "82ABNSEir_4b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHwjdjsir_4b",
        "outputId": "5e4c0c83-9794-47c4-bfbb-be15820db580"
      },
      "source": [
        "def generate_synthetic_gnss_imu(n_points=2000,\n",
        "                                spoof_windows=[(400,480),(1100,1180),(1600,1680)],\n",
        "                                abrupt_prob=0.6):\n",
        "    deg_to_m = 111320\n",
        "    lat = 51.75 + np.cumsum(np.random.normal(0, 0.00002, n_points))\n",
        "    lon = -1.25 + np.cumsum(np.random.normal(0, 0.00002, n_points))\n",
        "\n",
        "    dlat = np.concatenate([[0], np.diff(lat)])\n",
        "    dlon = np.concatenate([[0], np.diff(lon)])\n",
        "    disp_m = np.sqrt((dlat*deg_to_m)**2 + (dlon*deg_to_m*np.cos(np.deg2rad(lat)))**2)\n",
        "    speed = np.clip(disp_m, 0, None)\n",
        "    heading = (np.degrees(np.arctan2(dlon, np.where(dlat==0, 1e-9, dlat))) + 360) % 360\n",
        "\n",
        "    accel = np.concatenate([[0], np.diff(speed)]) + np.random.normal(0,0.02,n_points)\n",
        "    gyro = np.concatenate([[0], np.diff(heading)]) + np.random.normal(0,0.1,n_points)\n",
        "\n",
        "    gnss_lat = lat + np.random.normal(0, 1e-5, n_points)\n",
        "    gnss_lon = lon + np.random.normal(0, 1e-5, n_points)\n",
        "\n",
        "    spoofed = np.zeros(n_points, dtype=int)\n",
        "    for (s,e) in spoof_windows:\n",
        "        length = e - s\n",
        "        if np.random.rand() < abrupt_prob:\n",
        "            gnss_lat[s:e] += np.random.normal(0.0009, 0.0003)\n",
        "            gnss_lon[s:e] += np.random.normal(-0.0009, 0.0003)\n",
        "        else:\n",
        "            gnss_lat[s:e] += np.linspace(0, 0.0012, length) + np.random.normal(0,2e-5,length)\n",
        "            gnss_lon[s:e] += np.linspace(0, -0.0012, length) + np.random.normal(0,2e-5,length)\n",
        "        spoofed[s:e] = 1\n",
        "\n",
        "    snr = 30 + np.random.normal(0,1,n_points)\n",
        "    snr[spoofed==1] += np.random.normal(2.0,0.8, spoofed.sum())\n",
        "    sat_count = np.random.randint(6,12,n_points)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'time': np.arange(n_points),\n",
        "        'true_lat': lat, 'true_lon': lon,\n",
        "        'gnss_lat': gnss_lat, 'gnss_lon': gnss_lon,\n",
        "        'speed': speed, 'heading': heading,\n",
        "        'imu_ax': accel, 'imu_gyro': gyro,\n",
        "        'snr': snr, 'sat_count': sat_count,\n",
        "        'spoofed': spoofed\n",
        "    })\n",
        "    return df\n",
        "\n",
        "def load_or_generate_dataset():\n",
        "    return generate_synthetic_gnss_imu()\n",
        "\n",
        "print('Data generator ready')"
      ],
      "id": "xHwjdjsir_4b",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data generator ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DadOppemr_4c"
      },
      "source": [
        "## 4. Feature engineering\n",
        "Physics-informed features used by all models: GNSS displacement (m), INS proxy displacement, residuals, rolling stats, SNR, satellite count, heading change."
      ],
      "id": "DadOppemr_4c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns0XwCj2r_4c",
        "outputId": "9e03075d-bf34-438e-fe6a-fd7ba48c0970"
      },
      "source": [
        "def extract_features(df):\n",
        "    d = df.copy()\n",
        "    deg_to_m = 111320\n",
        "\n",
        "    d['dlat_g'] = d['gnss_lat'].diff().fillna(0)\n",
        "    d['dlon_g'] = d['gnss_lon'].diff().fillna(0)\n",
        "    d['dlat_m_g'] = d['dlat_g'] * deg_to_m\n",
        "    d['dlon_m_g'] = d['dlon_g'] * deg_to_m * np.cos(np.deg2rad(d['gnss_lat']))\n",
        "    d['disp_m_g'] = np.sqrt(d['dlat_m_g']**2 + d['dlon_m_g']**2)\n",
        "    d['disp_m_ins'] = d['speed']\n",
        "    d['disp_residual'] = d['disp_m_g'] - d['disp_m_ins']\n",
        "\n",
        "    for w in [3,5,11]:\n",
        "        d[f'disp_mean_{w}'] = d['disp_m_g'].rolling(w, min_periods=1).mean()\n",
        "        d[f'disp_std_{w}'] = d['disp_m_g'].rolling(w, min_periods=1).std().fillna(0)\n",
        "\n",
        "    d['dheading'] = np.abs(np.diff(np.pad(d['heading'].values,(1,0),'edge')))\n",
        "\n",
        "    features = ['gnss_lat','gnss_lon','speed','heading','disp_m_g','disp_m_ins',\n",
        "                'disp_residual','disp_mean_3','disp_std_3','disp_mean_5','disp_std_5',\n",
        "                'snr','sat_count','dheading']\n",
        "    features = [f for f in features if f in d.columns]\n",
        "    X = d[features].fillna(0)\n",
        "    y = d['spoofed'].astype(int).values\n",
        "    return X, y\n",
        "\n",
        "print('Feature extractor ready')"
      ],
      "id": "Ns0XwCj2r_4c",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extractor ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0uFeNpGr_4c"
      },
      "source": [
        "## 5. Metrics & plotting helpers\n",
        "Functions for common metrics and saved plots (confusion, ROC, metric bars, probability curves)."
      ],
      "id": "x0uFeNpGr_4c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gt6U7zJr_4c",
        "outputId": "5a3dbcb2-bc20-4d62-f0c4-f42aa22f8ccd"
      },
      "source": [
        "def safe_roc_auc(y_true, scores):\n",
        "    try:\n",
        "        return float(roc_auc_score(y_true, scores))\n",
        "    except Exception:\n",
        "        return float('nan')\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_score):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    auc = safe_roc_auc(y_true, y_score)\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"roc_auc\": auc}\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title, path):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.imshow(cm, cmap='Blues')\n",
        "    for (i,j), v in np.ndenumerate(cm):\n",
        "        ax.text(j, i, int(v), ha='center', va='center')\n",
        "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels(['Clean','Spoof']); ax.set_yticklabels(['Clean','Spoof'])\n",
        "    ax.set_xlabel('Predicted'); ax.set_ylabel('True'); ax.set_title(title)\n",
        "    fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n",
        "\n",
        "def plot_roc(y_true, y_prob, title, path):\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "        auc = safe_roc_auc(y_true, y_prob)\n",
        "    except Exception:\n",
        "        fpr, tpr, auc = [0,1], [0,1], float('nan')\n",
        "    fig, ax = plt.subplots(figsize=(4.5,3.5))\n",
        "    ax.plot(fpr, tpr, label=f'AUC={auc:.3f}')\n",
        "    ax.plot([0,1],[0,1],'--', color='gray')\n",
        "    ax.set_xlabel('FPR'); ax.set_ylabel('TPR'); ax.set_title(title); ax.legend()\n",
        "    fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n",
        "\n",
        "def plot_metrics_bar(metrics_dict, title, path):\n",
        "    names = list(metrics_dict.keys())\n",
        "    vals = list(metrics_dict.values())\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.bar(names, vals)\n",
        "    ax.set_ylim(0,1.05)\n",
        "    ax.set_title(title)\n",
        "    for i,v in enumerate(vals):\n",
        "        if isinstance(v, float) and not np.isnan(v):\n",
        "            ax.text(i, v+0.02, f'{v:.3f}', ha='center')\n",
        "        else:\n",
        "            ax.text(i, 0.5, 'n/a', ha='center')\n",
        "    fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n",
        "\n",
        "def plot_probabilities(proba_rf, proba_xgb, path):\n",
        "    fig, ax = plt.subplots(figsize=(10,2.8))\n",
        "    ax.plot(proba_xgb, label='XGBoost p(spoof)', alpha=0.9)\n",
        "    ax.plot(proba_rf, label='RandomForest p(spoof)', alpha=0.7)\n",
        "    ax.set_title('Predicted spoof probabilities (test set order)')\n",
        "    ax.set_xlabel('test sample index'); ax.set_ylabel('probability'); ax.legend()\n",
        "    fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n",
        "\n",
        "print('Metric & plotting helpers ready')"
      ],
      "id": "4Gt6U7zJr_4c",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric & plotting helpers ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPhzY787r_4d"
      },
      "source": [
        "## 6. LSTM Autoencoder implementation (sequence AE)\n",
        "Small, explainable LSTM AE. It trains on clean windows only and returns reconstruction error per sample by averaging windows covering each sample."
      ],
      "id": "KPhzY787r_4d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7xhqxCgr_4d",
        "outputId": "8a341c04-7863-4133-d283-da4cb5119fa5"
      },
      "source": [
        "class LSTMAE(nn.Module):\n",
        "    def __init__(self, n_features, emb_size=16):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(n_features, emb_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(emb_size, n_features, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F)\n",
        "        _, (h, _) = self.encoder(x)\n",
        "        z = h.permute(1, 0, 2)\n",
        "        z_rep = z.repeat(1, x.size(1), 1)\n",
        "        out, _ = self.decoder(z_rep)\n",
        "        return out\n",
        "\n",
        "def train_lstm_autoencoder(X, y, save_dir, seq_len=AE_SEQ_LEN, epochs=AE_EPOCHS, batch_size=AE_BATCH):\n",
        "    ensure_dir(save_dir)\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "\n",
        "    clean_mask = (y == 0)\n",
        "    arr = Xs[clean_mask]\n",
        "    if arr.shape[0] < seq_len + 1:\n",
        "        arr = Xs\n",
        "    seqs = []\n",
        "    for i in range(len(arr) - seq_len + 1):\n",
        "        seqs.append(arr[i:i+seq_len])\n",
        "    seqs = np.stack(seqs) if len(seqs) > 0 else np.empty((0, seq_len, Xs.shape[1]))\n",
        "    if seqs.shape[0] == 0:\n",
        "        raise RuntimeError('Not enough data to build AE sequences. Decrease seq_len or use more data.')\n",
        "\n",
        "    train_tensor = torch.tensor(seqs, dtype=torch.float32)\n",
        "    loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = LSTMAE(X.shape[1], emb_size=16)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    lossfn = nn.MSELoss()\n",
        "    losses = []\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        count = 0\n",
        "        for (batch,) in loader:\n",
        "            opt.zero_grad()\n",
        "            rec = model(batch)\n",
        "            loss = lossfn(rec, batch)\n",
        "            loss.backward(); opt.step()\n",
        "            running += loss.item() * batch.size(0)\n",
        "            count += batch.size(0)\n",
        "        epoch_loss = running / max(count, 1)\n",
        "        losses.append(epoch_loss)\n",
        "        print(f\"[AE] Epoch {ep}/{epochs} | loss={epoch_loss:.6f}\")\n",
        "    # Save loss curve\n",
        "    loss_path = os.path.join(save_dir, 'ae_loss.png')\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.plot(range(1, len(losses)+1), losses, marker='o')\n",
        "    ax.set_title('Autoencoder Training Loss'); ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
        "    fig.tight_layout(); fig.savefig(loss_path, dpi=150); plt.close(fig)\n",
        "\n",
        "    # compute per-window error across whole dataset\n",
        "    all_seqs = []\n",
        "    for i in range(len(Xs) - seq_len + 1):\n",
        "        all_seqs.append(Xs[i:i+seq_len])\n",
        "    all_seqs = np.stack(all_seqs)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        rec_all = model(torch.tensor(all_seqs, dtype=torch.float32)).numpy()\n",
        "    rec_err = np.mean((rec_all - all_seqs)**2, axis=(1,2))\n",
        "\n",
        "    # map window errors to sample-level by averaging overlapping windows\n",
        "    err_per_sample = np.zeros(len(Xs))\n",
        "    counts = np.zeros(len(Xs))\n",
        "    for i, e in enumerate(rec_err):\n",
        "        err_per_sample[i:i+seq_len] += e\n",
        "        counts[i:i+seq_len] += 1\n",
        "    counts[counts == 0] = 1\n",
        "    err_per_sample = err_per_sample / counts\n",
        "\n",
        "    thresh = np.percentile(err_per_sample[clean_mask] if np.any(clean_mask) else err_per_sample, 95)\n",
        "    preds = (err_per_sample > thresh).astype(int)\n",
        "\n",
        "    ae_metrics = compute_metrics(y, preds, err_per_sample)\n",
        "\n",
        "    err_plot_path = os.path.join(save_dir, 'ae_error_plot.png')\n",
        "    fig, ax = plt.subplots(figsize=(8,3))\n",
        "    ax.plot(err_per_sample, label='Reconstruction error')\n",
        "    ax.axhline(thresh, color='r', ls='--', label='95% threshold')\n",
        "    ax.scatter(np.where(y==1)[0], err_per_sample[y==1], c='orange', s=10, label='true spoof')\n",
        "    ax.set_title('Autoencoder Reconstruction Errors per sample')\n",
        "    ax.set_xlabel('sample index'); ax.set_ylabel('MSE'); ax.legend()\n",
        "    fig.tight_layout(); fig.savefig(err_plot_path, dpi=150); plt.close(fig)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'scaler': scaler,\n",
        "        'errors': err_per_sample,\n",
        "        'threshold': thresh,\n",
        "        'losses': losses,\n",
        "        'metrics': ae_metrics,\n",
        "        'loss_path': loss_path,\n",
        "        'err_plot_path': err_plot_path\n",
        "    }\n",
        "\n",
        "print('AE implementation ready')"
      ],
      "id": "Z7xhqxCgr_4d",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AE implementation ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXsvnRwdr_4d"
      },
      "source": [
        "## 7. Cross-validation routine (RF & XGB)\n",
        "- Runs Stratified K-Fold CV.\n",
        "- Saves per-fold metrics CSVs and saves visuals **only for the final fold** (fold `N_SPLITS`) per your instruction."
      ],
      "id": "WXsvnRwdr_4d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNDLY5tIr_4e",
        "outputId": "5d91a012-78eb-4bf1-d1fd-27181f18dcc4"
      },
      "source": [
        "def run_cross_validation(X, y, out_root, n_splits=5):\n",
        "    ensure_dir(out_root)\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    rows = []\n",
        "    last_fold_visuals = {}\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
        "        print(f'\\n--- Fold {fold_idx}/{n_splits} ---')\n",
        "        Xtr, Xte = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        ytr, yte = y[train_idx], y[test_idx]\n",
        "\n",
        "        rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED)\n",
        "        rf.fit(Xtr.values, ytr)\n",
        "        rf_pred = rf.predict(Xte.values)\n",
        "        rf_proba = rf.predict_proba(Xte.values)[:,1]\n",
        "        rf_m = compute_metrics(yte, rf_pred, rf_proba)\n",
        "        rows.append({'model':'RandomForest','fold':fold_idx, **rf_m})\n",
        "\n",
        "        xg = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_SEED)\n",
        "        xg.fit(Xtr.values, ytr)\n",
        "        xg_pred = xg.predict(Xte.values)\n",
        "        xg_proba = xg.predict_proba(Xte.values)[:,1]\n",
        "        xg_m = compute_metrics(yte, xg_pred, xg_proba)\n",
        "        rows.append({'model':'XGBoost','fold':fold_idx, **xg_m})\n",
        "\n",
        "        if fold_idx == n_splits:\n",
        "            fold_dir = os.path.join(out_root, f'cv_fold_{fold_idx}')\n",
        "            ensure_dir(fold_dir)\n",
        "            print('Saving visuals for final fold to:', fold_dir)\n",
        "            plot_confusion(yte, rf_pred, f'RF Confusion (fold {fold_idx})', os.path.join(fold_dir, 'rf_conf_lastfold.png'))\n",
        "            plot_roc(yte, rf_proba, f'RF ROC (fold {fold_idx})', os.path.join(fold_dir, 'rf_roc_lastfold.png'))\n",
        "            plot_metrics_bar(rf_m, f'RF Metrics (fold {fold_idx})', os.path.join(fold_dir, 'rf_metrics_lastfold.png'))\n",
        "\n",
        "            plot_confusion(yte, xg_pred, f'XGB Confusion (fold {fold_idx})', os.path.join(fold_dir, 'xgb_conf_lastfold.png'))\n",
        "            plot_roc(yte, xg_proba, f'XGB ROC (fold {fold_idx})', os.path.join(fold_dir, 'xgb_roc_lastfold.png'))\n",
        "            plot_metrics_bar(xg_m, f'XGB Metrics (fold {fold_idx})', os.path.join(fold_dir, 'xgb_metrics_lastfold.png'))\n",
        "            last_fold_visuals = {\n",
        "                'rf_conf': os.path.join(fold_dir, 'rf_conf_lastfold.png'),\n",
        "                'rf_roc': os.path.join(fold_dir, 'rf_roc_lastfold.png'),\n",
        "                'rf_metrics': os.path.join(fold_dir, 'rf_metrics_lastfold.png'),\n",
        "                'xgb_conf': os.path.join(fold_dir, 'xgb_conf_lastfold.png'),\n",
        "                'xgb_roc': os.path.join(fold_dir, 'xgb_roc_lastfold.png'),\n",
        "                'xgb_metrics': os.path.join(fold_dir, 'xgb_metrics_lastfold.png'),\n",
        "                'fold_idx': fold_idx\n",
        "            }\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(os.path.join(out_root, 'cv_per_fold_metrics.csv'), index=False)\n",
        "    avg = df.groupby('model')[['accuracy','precision','recall','f1','roc_auc']].mean().reset_index()\n",
        "    avg.to_csv(os.path.join(out_root, 'cv_average_metrics.csv'), index=False)\n",
        "    return avg, df, last_fold_visuals\n",
        "\n",
        "print('Cross-validation routine ready')"
      ],
      "id": "JNDLY5tIr_4e",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation routine ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNvoCOK2r_4e"
      },
      "source": [
        "## 8. Unified pipeline (run everything) — documentation inside\n",
        "This main function runs the dataset generation, feature extraction, split evaluation (80:20), CV (k-fold), AE training, and generates a single PDF that contains:\n",
        "- Split results (tables and plots)\n",
        "- CV average metrics and last-fold visuals\n",
        "- AE results (loss + errors + threshold + AE metrics)\n",
        "- A combined comparison table and summary chart"
      ],
      "id": "sNvoCOK2r_4e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSAhNNBNr_4e",
        "outputId": "48a01333-c901-44be-b05d-0e4319623dd6"
      },
      "source": [
        "def run_unified_pipeline(n_points=2000):\n",
        "    # 1) Data\n",
        "    print('1) Generating synthetic dataset...')\n",
        "    df = generate_synthetic_gnss_imu(n_points=n_points)\n",
        "    print('   dataset shape:', df.shape)\n",
        "\n",
        "    # 2) Features\n",
        "    print('2) Extracting features...')\n",
        "    X, y = extract_features(df)\n",
        "    print('   features shape:', X.shape, '| labels shape:', y.shape)\n",
        "\n",
        "    # A: 80:20 train/test\n",
        "    print('\\n=== A: 80:20 train/test evaluation ===')\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED)\n",
        "    rf.fit(Xtr.values, ytr)\n",
        "    rf_pred = rf.predict(Xte.values)\n",
        "    rf_proba = rf.predict_proba(Xte.values)[:,1]\n",
        "    rf_metrics = compute_metrics(yte, rf_pred, rf_proba)\n",
        "    print('RF (80:20) metrics:', rf_metrics)\n",
        "\n",
        "    xg = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_SEED)\n",
        "    xg.fit(Xtr.values, ytr)\n",
        "    xg_pred = xg.predict(Xte.values)\n",
        "    xg_proba = xg.predict_proba(Xte.values)[:,1]\n",
        "    xg_metrics = compute_metrics(yte, xg_pred, xg_proba)\n",
        "    print('XGB (80:20) metrics:', xg_metrics)\n",
        "\n",
        "    # Save split visuals\n",
        "    ensure_dir(FIGS_DIR)\n",
        "    plot_confusion(yte, rf_pred, 'RF Confusion (80:20)', os.path.join(FIGS_DIR, 'rf_conf_80_20.png'))\n",
        "    plot_confusion(yte, xg_pred, 'XGB Confusion (80:20)', os.path.join(FIGS_DIR, 'xgb_conf_80_20.png'))\n",
        "    plot_roc(yte, rf_proba, 'RF ROC (80:20)', os.path.join(FIGS_DIR, 'rf_roc_80_20.png'))\n",
        "    plot_roc(yte, xg_proba, 'XGB ROC (80:20)', os.path.join(FIGS_DIR, 'xgb_roc_80_20.png'))\n",
        "    plot_metrics_bar(rf_metrics, 'RF Metrics (80:20)', os.path.join(FIGS_DIR, 'rf_metrics_80_20.png'))\n",
        "    plot_metrics_bar(xg_metrics, 'XGB Metrics (80:20)', os.path.join(FIGS_DIR, 'xgb_metrics_80_20.png'))\n",
        "    plot_probabilities(rf_proba, xg_proba, os.path.join(FIGS_DIR, 'probabilities_80_20.png'))\n",
        "\n",
        "    # B: Cross-validation\n",
        "    print('\\n=== B: Stratified K-Fold Cross-Validation (k={}) ==='.format(N_SPLITS))\n",
        "    avg_cv, df_cv, last_fold_visuals = run_cross_validation(X, y, FIGS_DIR, N_SPLITS)\n",
        "    print('\\nAverage CV metrics:\\n', avg_cv)\n",
        "\n",
        "    # C: Autoencoder\n",
        "    ae_info = None\n",
        "    if DO_AE:\n",
        "        print('\\n=== C: LSTM Autoencoder (unsupervised) ===')\n",
        "        ae_info = train_lstm_autoencoder(X.values, y, AE_DIR, seq_len=AE_SEQ_LEN, epochs=AE_EPOCHS, batch_size=AE_BATCH)\n",
        "        print('AE threshold (95th percentile on clean):', ae_info['threshold'])\n",
        "        print('AE metrics (thresholded labels):', ae_info['metrics'])\n",
        "    else:\n",
        "        print('Skipping AE (DO_AE=False)')\n",
        "\n",
        "    # D: Build comparison table and charts\n",
        "    print('\\n=== D: Building comparison tables and summary charts ===')\n",
        "    split_rows = [\n",
        "        {'model': 'RandomForest (80:20)', **rf_metrics},\n",
        "        {'model': 'XGBoost (80:20)', **xg_metrics}\n",
        "    ]\n",
        "    if ae_info:\n",
        "        split_rows.append({'model': 'LSTM-AE (thresholded)', **ae_info['metrics']})\n",
        "    else:\n",
        "        split_rows.append({'model': 'LSTM-AE (thresholded)', 'accuracy':np.nan, 'precision':np.nan, 'recall':np.nan, 'f1':np.nan, 'roc_auc':np.nan})\n",
        "\n",
        "    split_df = pd.DataFrame(split_rows)\n",
        "    split_df.to_csv(os.path.join(OUT_ROOT, 'split_results.csv'), index=False)\n",
        "\n",
        "    # compose comparison DataFrame that includes CV averages\n",
        "    comp_df = split_df.copy()\n",
        "    for _, row in avg_cv.iterrows():\n",
        "        comp_df = pd.concat([\n",
        "            comp_df,\n",
        "            pd.DataFrame([{ 'model': f\"{row['model']} (CV avg)\", 'accuracy': row['accuracy'], 'precision': row['precision'], 'recall': row['recall'], 'f1': row['f1'], 'roc_auc': row['roc_auc'] }])\n",
        "        ], ignore_index=True)\n",
        "\n",
        "    # Save comp chart\n",
        "    summary_chart_path = os.path.join(FIGS_DIR, 'summary_comparison.png')\n",
        "    metrics = ['accuracy','precision','recall','f1','roc_auc']\n",
        "    fig, ax = plt.subplots(figsize=(10,3.5))\n",
        "    n = len(comp_df)\n",
        "    x = np.arange(n)\n",
        "    width = 0.12\n",
        "    for i,m in enumerate(metrics):\n",
        "        vals = comp_df[m].astype(float).values\n",
        "        ax.bar(x + i*width, vals, width, label=m)\n",
        "    ax.set_xticks(x + width*2)\n",
        "    ax.set_xticklabels(comp_df['model'], rotation=30, ha='right', fontsize=8)\n",
        "    ax.set_ylim(0,1.05)\n",
        "    ax.legend(ncol=3, fontsize=8)\n",
        "    ax.set_title('Model Comparison: Split vs CV vs AE (thresholded)')\n",
        "    fig.tight_layout(); fig.savefig(summary_chart_path, dpi=150); plt.close(fig)\n",
        "\n",
        "    # Save comparison table image\n",
        "    def save_table_image(df_table, path, title=''):\n",
        "        fig, ax = plt.subplots(figsize=(10, 1.2 + 0.35*len(df_table)))\n",
        "        ax.axis('off')\n",
        "        tbl = ax.table(cellText=df_table.round(3).values, colLabels=df_table.columns, loc='center')\n",
        "        tbl.auto_set_font_size(False); tbl.set_fontsize(9); tbl.scale(1,1.2)\n",
        "        if title:\n",
        "            ax.set_title(title)\n",
        "        fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n",
        "\n",
        "    table_img_path = os.path.join(FIGS_DIR, 'comparison_table.png')\n",
        "    save_table_image(comp_df[['model'] + metrics], table_img_path, title='Split & CV Comparison (selected metrics)')\n",
        "\n",
        "    # E: Build unified PDF\n",
        "    print('\\n=== E: Generating unified PDF report ===')\n",
        "    pdf_path = os.path.join(OUT_ROOT, 'unified_report.pdf')\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elems = []\n",
        "    elems.append(Paragraph('Unified GPS & IMU Spoofing Detection Report', styles['Title']))\n",
        "    elems.append(Spacer(1, 8))\n",
        "    elems.append(Paragraph(f'Generated: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")}', styles['Normal']))\n",
        "    elems.append(Spacer(1, 12))\n",
        "    # A: split section\n",
        "    elems.append(Paragraph('A. Train/Test Split (80:20) Results', styles['Heading2']))\n",
        "    elems.append(Spacer(1,6))\n",
        "    elems.append(Paragraph('Metrics (80:20): RandomForest & XGBoost. LSTM Autoencoder thresholded included for comparison.', styles['Normal']))\n",
        "    elems.append(Spacer(1,6))\n",
        "    if os.path.exists(table_img_path):\n",
        "        elems.append(Image(table_img_path, width=500, height=140)); elems.append(Spacer(1,8))\n",
        "    for img in ['rf_conf_80_20.png','xgb_conf_80_20.png','rf_roc_80_20.png','xgb_roc_80_20.png','rf_metrics_80_20.png','xgb_metrics_80_20.png','probabilities_80_20.png']:\n",
        "        ip = os.path.join(FIGS_DIR, img)\n",
        "        if os.path.exists(ip):\n",
        "            elems.append(Image(ip, width=420, height=200)); elems.append(Spacer(1,6))\n",
        "\n",
        "    # B: CV\n",
        "    elems.append(Paragraph(f'B. Stratified K-Fold Cross-Validation (k={N_SPLITS})', styles['Heading2']))\n",
        "    elems.append(Spacer(1,6))\n",
        "    elems.append(Paragraph('Average cross-validation metrics (grouped by model). Only the final fold visuals are included below.', styles['Normal']))\n",
        "    elems.append(Spacer(1,6))\n",
        "    try:\n",
        "        elems.append(Paragraph(avg_cv.to_html(index=False), styles['Normal']))\n",
        "    except Exception:\n",
        "        elems.append(Paragraph(str(avg_cv), styles['Normal']))\n",
        "    elems.append(Spacer(1,8))\n",
        "    if last_fold_visuals:\n",
        "        elems.append(Paragraph(f'Visuals from last fold (fold {last_fold_visuals.get(\"fold_idx\", N_SPLITS)})', styles['Heading3']))\n",
        "        for key in ['rf_conf','rf_roc','rf_metrics','xgb_conf','xgb_roc','xgb_metrics']:\n",
        "            ip = last_fold_visuals.get(key)\n",
        "            if ip and os.path.exists(ip):\n",
        "                elems.append(Image(ip, width=420, height=200)); elems.append(Spacer(1,6))\n",
        "\n",
        "    # C: AE\n",
        "    if ae_info:\n",
        "        elems.append(Paragraph('C. LSTM Autoencoder (unsupervised) Results', styles['Heading2']))\n",
        "        elems.append(Spacer(1,6))\n",
        "        elems.append(Paragraph(f'Threshold (95th percentile on clean): {ae_info[\"threshold\"]:.6f}', styles['Normal']))\n",
        "        elems.append(Spacer(1,6))\n",
        "        try:\n",
        "            elems.append(Paragraph(pd.DataFrame([ae_info['metrics']], index=['LSTM-AE']).to_html(), styles['Normal']))\n",
        "        except Exception:\n",
        "            elems.append(Paragraph(str(ae_info['metrics']), styles['Normal']))\n",
        "        elems.append(Spacer(1,6))\n",
        "        if os.path.exists(ae_info.get('loss_path','')):\n",
        "            elems.append(Image(ae_info['loss_path'], width=420, height=200)); elems.append(Spacer(1,6))\n",
        "        if os.path.exists(ae_info.get('err_plot_path','')):\n",
        "            elems.append(Image(ae_info['err_plot_path'], width=420, height=140)); elems.append(Spacer(1,6))\n",
        "\n",
        "    # D: final comparison\n",
        "    elems.append(Paragraph('D. Final Comparison & Summary', styles['Heading2']))\n",
        "    elems.append(Spacer(1,6))\n",
        "    if os.path.exists(summary_chart_path):\n",
        "        elems.append(Image(summary_chart_path, width=500, height=220)); elems.append(Spacer(1,6))\n",
        "\n",
        "    doc.build(elems)\n",
        "    print('\\n✅ Unified PDF report written to:', pdf_path)\n",
        "    print('All figures saved to:', FIGS_DIR, '; AE outputs saved to:', AE_DIR)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_unified_pipeline(n_points=2000)\n"
      ],
      "id": "DSAhNNBNr_4e",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Generating synthetic dataset...\n",
            "   dataset shape: (2000, 12)\n",
            "2) Extracting features...\n",
            "   features shape: (2000, 14) | labels shape: (2000,)\n",
            "\n",
            "=== A: 80:20 train/test evaluation ===\n",
            "RF (80:20) metrics: {'accuracy': 0.99, 'precision': 1.0, 'recall': 0.9166666666666666, 'f1': 0.9565217391304348, 'roc_auc': 0.9995857007575757}\n",
            "XGB (80:20) metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:32:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== B: Stratified K-Fold Cross-Validation (k=5) ===\n",
            "\n",
            "--- Fold 1/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:32:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 2/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:32:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 3/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:32:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 4/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:32:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 5/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:32:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving visuals for final fold to: outputs_combined/figures/cv_fold_5\n",
            "\n",
            "Average CV metrics:\n",
            "           model  accuracy  precision    recall        f1   roc_auc\n",
            "0  RandomForest    0.9950   0.991667  0.966667  0.978899  0.999657\n",
            "1       XGBoost    0.9985   1.000000  0.987500  0.993684  0.998923\n",
            "\n",
            "=== C: LSTM Autoencoder (unsupervised) ===\n",
            "[AE] Epoch 1/15 | loss=0.739879\n",
            "[AE] Epoch 2/15 | loss=0.698526\n",
            "[AE] Epoch 3/15 | loss=0.671854\n",
            "[AE] Epoch 4/15 | loss=0.661221\n",
            "[AE] Epoch 5/15 | loss=0.651356\n",
            "[AE] Epoch 6/15 | loss=0.639789\n",
            "[AE] Epoch 7/15 | loss=0.632340\n",
            "[AE] Epoch 8/15 | loss=0.628165\n",
            "[AE] Epoch 9/15 | loss=0.624925\n",
            "[AE] Epoch 10/15 | loss=0.621833\n",
            "[AE] Epoch 11/15 | loss=0.619374\n",
            "[AE] Epoch 12/15 | loss=0.617776\n",
            "[AE] Epoch 13/15 | loss=0.616567\n",
            "[AE] Epoch 14/15 | loss=0.615708\n",
            "[AE] Epoch 15/15 | loss=0.614489\n",
            "AE threshold (95th percentile on clean): 1.1607283979561749\n",
            "AE metrics (thresholded labels): {'accuracy': 0.9145, 'precision': 0.6408163265306123, 'recall': 0.6541666666666667, 'f1': 0.6474226804123712, 'roc_auc': 0.9536647727272728}\n",
            "\n",
            "=== D: Building comparison tables and summary charts ===\n",
            "\n",
            "=== E: Generating unified PDF report ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2502685337.py:115: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  elems.append(Paragraph(f'Generated: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")}', styles['Normal']))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Unified PDF report written to: outputs_combined/unified_report.pdf\n",
            "All figures saved to: outputs_combined/figures ; AE outputs saved to: outputs_combined/autoencoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfvSuSY_r_4f"
      },
      "source": [
        "## 9. Notes (Implementation details & reproducibility)\n",
        "\n",
        "- **Data preprocessing:** feature extraction uses GNSS diff -> meters conversion and rolling windows (3,5,11). Missing values are filled with 0 for simplicity; for real data consider interpolation and more careful handling.\n",
        "- **Hyperparameters:** default tree counts: `n_estimators=200`. AE: `seq_len=20`, `epochs=15`. These were chosen for speed. For publication use randomized/grid search and report ranges.\n",
        "- **AE threshold:** 95th percentile on clean reconstruction errors (common heuristic). You may calibrate threshold using validation sets.\n",
        "- **What the PDF contains:** Split results (80:20), CV average metrics (and last-fold visuals only), AE results, final comparison chart and comparison table.\n",
        "- **Run-time:** AE training is CPU-friendly but faster on GPU.\n",
        "\n",
        "If you'd like, I can also produce:\n",
        "- a) a ZIP with `merged_spoofing_pipeline.ipynb` and `outputs_combined/` (if you want me to run and upload outputs), or\n",
        "- b) a reduced notebook without AE for faster runs."
      ],
      "id": "nfvSuSY_r_4f"
    }
  ]
}