{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "e6a519c8", "cell_type": "markdown", "source": "# Detection of GPS and IMU Spoofing in Autonomous Systems (Research-style Notebook)\n\n**Author:** Badrudeen Yusuf Akinkunmi  \n**Date:** 2025-08-11\n\n**Abstract:**  \nThis notebook demonstrates an end-to-end research-style pipeline for detecting GPS (GNSS) spoofing using synthetic GNSS + IMU data. We generate realistic synthetic trajectories, inject configurable spoofing events (abrupt jumps and gradual drifts), extract physics-informed features, and train two supervised classifiers \u2014 **XGBoost** and **Random Forest** \u2014 for spoof detection. For completeness we also include an optional LSTM Autoencoder section (unsupervised) and an appendix explaining how to plug in real datasets (Kaggle/Oxford RobotCar, TEXBAT).\n\n**Notebook notes:**  \n- Plots and metrics are produced only when code cells are executed.  \n- The notebook is Colab-ready; pip install commands are commented where appropriate.  \n", "metadata": {}}, {"id": "b6b86e8d", "cell_type": "markdown", "source": "## 1. Introduction\n\nGPS spoofing is the intentional transmission of counterfeit GNSS signals to deceive a receiver about its true position or time. For autonomous vehicles, successful spoofing can cause incorrect navigation, unsafe maneuvers, or loss of situational awareness.\n\nThis notebook explores:\n- Synthetic data generation for safe, repeatable experiments.\n- Feature engineering combining GNSS and IMU signals.\n- Supervised learning comparison (XGBoost vs Random Forest).\n- Optional unsupervised anomaly detection with an LSTM Autoencoder.\n- A simple mitigation demonstration that reduces reliance on GNSS when spoofing is detected.\n", "metadata": {}}, {"id": "a0622e8b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Setup & Imports ===\n# Uncomment the install line in Colab if packages are not already available:\n# !pip install xgboost scikit-learn pandas numpy matplotlib seaborn torch --quiet\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (classification_report, confusion_matrix,\n                             roc_auc_score, precision_recall_fscore_support,\n                             accuracy_score)\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nimport random\nimport os\n\nsns.set(style='whitegrid')\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n", "outputs": []}, {"id": "43d78577", "cell_type": "markdown", "source": "## 2. Methodology (Overview)\n\n**Synthetic data generation**\n- Create a smooth random-walk \"true\" trajectory (latitude, longitude).\n- Create GNSS-reported coordinates by adding small receiver noise.\n- Inject spoofing in configurable windows:\n  - Abrupt jump (large instantaneous offset).\n  - Gradual drift (small offset accumulating over time).\n- Create simple IMU proxies (accelerometer and gyroscope signals) that remain *truthful* (not spoofed).\n\n**Feature engineering**\n- GNSS displacement (meters) between consecutive samples.\n- INS proxy displacement (from speed).\n- Residuals between GNSS and INS-derived displacement.\n- Rolling statistics (mean/std) of displacement.\n- Synthetic SNR and satellite count features.\n\n**Models**\n- XGBoost classifier (supervised).\n- Random Forest classifier (supervised).\n- Optional LSTM Autoencoder for unsupervised anomaly detection.\n\n**Mitigation (demo)**\n- A toy fusion that reduces trust in GNSS when detector flags spoofing (visual demonstration).\n", "metadata": {}}, {"id": "6d3255c3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Synthetic GNSS + IMU Generator ===\ndef generate_synthetic_gnss_imu(n_points=2000,\n                                spoof_windows=[(400,480),(1100,1180),(1600,1680)],\n                                abrupt_prob=0.6):\n    \"\"\"Generate synthetic GNSS+IMU dataframe with configurable spoof windows.\"\"\"\n    deg_to_m = 111320\n    # True trajectory (small random walk)\n    lat = 51.75 + np.cumsum(np.random.normal(0, 0.00002, n_points))\n    lon = -1.25 + np.cumsum(np.random.normal(0, 0.00002, n_points))\n    # per-step displacement approx (meters)\n    dlat = np.concatenate([[0], np.diff(lat)])\n    dlon = np.concatenate([[0], np.diff(lon)])\n    disp_m = np.sqrt((dlat*deg_to_m)**2 + (dlon*deg_to_m*np.cos(np.deg2rad(lat)))**2)\n    speed = np.clip(disp_m, 0, None)  # proxy for per-timestep motion\n    heading = (np.degrees(np.arctan2(dlon, np.where(dlat==0, 1e-9, dlat))) + 360) % 360\n    # IMU proxies\n    accel = np.concatenate([[0], np.diff(speed)]) + np.random.normal(0,0.02,n_points)\n    gyro = np.concatenate([[0], np.diff(heading)]) + np.random.normal(0,0.1,n_points)\n    # GNSS reported (true + receiver noise)\n    gnss_lat = lat + np.random.normal(0, 1e-5, n_points)\n    gnss_lon = lon + np.random.normal(0, 1e-5, n_points)\n    spoofed = np.zeros(n_points, dtype=int)\n    # Inject spoofing windows\n    for (s,e) in spoof_windows:\n        length = e - s\n        if np.random.rand() < abrupt_prob:\n            lat_off = np.random.normal(0.0009, 0.0003)\n            lon_off = np.random.normal(-0.0009, 0.0003)\n            gnss_lat[s:e] += lat_off\n            gnss_lon[s:e] += lon_off\n        else:\n            gnss_lat[s:e] += np.linspace(0, 0.0012, length) + np.random.normal(0, 2e-5, length)\n            gnss_lon[s:e] += np.linspace(0, -0.0012, length) + np.random.normal(0, 2e-5, length)\n        spoofed[s:e] = 1\n    # synthetic SNR and satellite count\n    snr = 30 + np.random.normal(0,1,n_points)\n    snr[spoofed==1] += np.random.normal(2.0,0.8, spoofed.sum())\n    sat_count = np.random.randint(6,12,n_points)\n    df = pd.DataFrame({\n        'time': np.arange(n_points),\n        'true_lat': lat, 'true_lon': lon,\n        'gnss_lat': gnss_lat, 'gnss_lon': gnss_lon,\n        'speed': speed, 'heading': heading,\n        'imu_ax': accel, 'imu_gyro': gyro,\n        'snr': snr, 'sat_count': sat_count,\n        'spoofed': spoofed\n    })\n    return df\n\n# Generate default dataset\ndf = generate_synthetic_gnss_imu()\nprint('Synthetic dataset created:', df.shape)\ndf.head()\n", "outputs": []}, {"id": "dbc80ce5", "cell_type": "markdown", "source": "### Feature Engineering\n\nWe extract the following features per time step:\n- **GNSS displacement (meters)** between consecutive GNSS samples.\n- **INS displacement proxy** from speed.\n- **Residual**: GNSS_disp - INS_disp (useful to detect inconsistency).\n- **Rolling mean/std** of GNSS displacement (windows = 3,5,11).\n- **SNR** and **satellite count** (synthetic here).\n- **Heading change** magnitude.\n", "metadata": {}}, {"id": "3d7aab95", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Feature extraction ===\ndef extract_features(df):\n    d = df.copy()\n    deg_to_m = 111320\n    d['dlat_g'] = d['gnss_lat'].diff().fillna(0)\n    d['dlon_g'] = d['gnss_lon'].diff().fillna(0)\n    d['dlat_m_g'] = d['dlat_g'] * deg_to_m\n    d['dlon_m_g'] = d['dlon_g'] * deg_to_m * np.cos(np.deg2rad(d['gnss_lat']))\n    d['disp_m_g'] = np.sqrt(d['dlat_m_g']**2 + d['dlon_m_g']**2)\n    d['disp_m_ins'] = d['speed']\n    d['disp_residual'] = d['disp_m_g'] - d['disp_m_ins']\n    for w in [3,5,11]:\n        d[f'disp_mean_{w}'] = d['disp_m_g'].rolling(w, min_periods=1).mean()\n        d[f'disp_std_{w}'] = d['disp_m_g'].rolling(w, min_periods=1).std().fillna(0)\n    d['dheading'] = np.abs(np.diff(np.pad(d['heading'].values,(1,0),'edge')))\n    features = ['gnss_lat','gnss_lon','speed','heading','disp_m_g','disp_m_ins',\n                'disp_residual','disp_mean_3','disp_std_3','disp_mean_5','disp_std_5',\n                'snr','sat_count','dheading']\n    features = [f for f in features if f in d.columns]\n    X = d[features].fillna(0)\n    y = d['spoofed'].astype(int).values\n    return X, y\n\nX, y = extract_features(df)\nprint('Features shape:', X.shape)\nX.head()\n", "outputs": []}, {"id": "e4e6843d", "cell_type": "markdown", "source": "## Model Training\n\nWe split the data into train/test (80/20) stratified by the spoof label, then train:\n- **XGBoost** (gradient-boosted trees) \u2014 fast + strong baseline.\n- **Random Forest** \u2014 ensemble of decision trees, robust and interpretable.\n\nWe will compare metrics (accuracy, precision, recall, F1-score, ROC-AUC) and confusion matrices.\n", "metadata": {}}, {"id": "2620b28b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Train/Test split ===\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=RANDOM_SEED, stratify=y)\n\n# XGBoost (no deprecated param)\nxgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=RANDOM_SEED)\nxgb_model.fit(X_train, y_train)\nxgb_pred = xgb_model.predict(X_test)\nxgb_proba = xgb_model.predict_proba(X_test)[:,1]\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=200, max_depth=8, random_state=RANDOM_SEED)\nrf_model.fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\nrf_proba = rf_model.predict_proba(X_test)[:,1]\n\nprint('Training complete.')", "outputs": []}, {"id": "f209079c", "cell_type": "markdown", "source": "## Results & Comparison\n\nBelow we compute standard classification metrics and present a comparison table for XGBoost vs Random Forest.\n", "metadata": {}}, {"id": "9ba1f4ed", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Metrics computation ===\ndef compute_metrics(y_true, y_pred, y_proba):\n    acc = accuracy_score(y_true, y_pred)\n    prec, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n    auc = roc_auc_score(y_true, y_proba)\n    return {'accuracy': acc, 'precision': prec, 'recall': recall, 'f1': f1, 'roc_auc': auc}\n\nxgb_metrics = compute_metrics(y_test, xgb_pred, xgb_proba)\nrf_metrics = compute_metrics(y_test, rf_pred, rf_proba)\nmetrics_df = pd.DataFrame([xgb_metrics, rf_metrics], index=['XGBoost','RandomForest'])\nmetrics_df\n", "outputs": []}, {"id": "9fdddfed", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Confusion Matrices ===\nfig, axs = plt.subplots(1,2, figsize=(10,4))\ncm_x = confusion_matrix(y_test, xgb_pred)\ncm_r = confusion_matrix(y_test, rf_pred)\nsns.heatmap(cm_x, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('XGBoost Confusion Matrix'); axs[0].set_xlabel('Predicted'); axs[0].set_ylabel('Actual')\nsns.heatmap(cm_r, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('Random Forest Confusion Matrix'); axs[1].set_xlabel('Predicted'); axs[1].set_ylabel('Actual')\nplt.tight_layout()\nplt.show()\n", "outputs": []}, {"id": "8299c071", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Visualizations ===\n# 1) GNSS track with true spoof points\nplt.figure(figsize=(8,6))\nplt.scatter(df['gnss_lon'], df['gnss_lat'], c='lightgray', s=8, label='GNSS raw')\nplt.scatter(df.loc[df['spoofed']==1,'gnss_lon'], df.loc[df['spoofed']==1,'gnss_lat'], facecolors='none', edgecolors='blue', s=60, label='True spoof')\nplt.title('Synthetic GNSS Track (True spoof points circled)'); plt.xlabel('longitude'); plt.ylabel('latitude')\nplt.legend(); plt.show()\n\n# 2) Model probabilities on test set (aligned by index)\nplt.figure(figsize=(12,3))\nplt.plot(xgb_proba, label='XGBoost p(spoof)', alpha=0.9)\nplt.plot(rf_proba, label='RandomForest p(spoof)', alpha=0.7)\nplt.title('Predicted spoof probabilities (test set)'); plt.legend(); plt.xlabel('test sample index'); plt.ylabel('probability')\nplt.show()\n\n# 3) Bar chart comparison of metrics\nmetrics_to_plot = metrics_df[['precision','recall','f1','accuracy']]\nmetrics_to_plot.plot(kind='bar', figsize=(8,5))\nplt.title('Model comparison (precision, recall, f1, accuracy)')\nplt.ylim(0,1)\nplt.legend(loc='lower right')\nplt.show()\n", "outputs": []}, {"id": "70aa73d9", "cell_type": "markdown", "source": "## Optional: LSTM Autoencoder (Unsupervised) \u2014 Brief\n\nBelow is an optional section to train a sequence LSTM autoencoder on *clean* windows and use reconstruction error as an anomaly score. This is useful when labeled spoof examples are scarce.\n\nRun the cell if you want to include an unsupervised detector in addition to the two supervised models.\n", "metadata": {}}, {"id": "350782cc", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === Optional LSTM Autoencoder (train only if desired) ===\ndo_ae = True  # set False to skip the autoencoder section\nif do_ae:\n    # Prepare scaled sequences\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    SEQ = 20\n    def create_sequences(Xarr, seq_len=SEQ):\n        seqs = []\n        for i in range(len(Xarr)-seq_len+1):\n            seqs.append(Xarr[i:i+seq_len])\n        return np.stack(seqs)\n    clean_mask = (y==0)\n    X_clean_seq = create_sequences(X_scaled[clean_mask], SEQ)\n    if X_clean_seq.shape[0] < 10:\n        X_clean_seq = create_sequences(X_scaled, SEQ)  # fallback\n    train_tensor = torch.tensor(X_clean_seq, dtype=torch.float32)\n    train_loader = DataLoader(TensorDataset(train_tensor), batch_size=64, shuffle=True)\n    # Define a small LSTM AE\n    class LSTMAE(nn.Module):\n        def __init__(self, n_features, hidden=64, latent=16):\n            super().__init__()\n            self.enc = nn.LSTM(n_features, hidden, batch_first=True)\n            self.enc_fc = nn.Linear(hidden, latent)\n            self.dec_fc = nn.Linear(latent, hidden)\n            self.dec = nn.LSTM(hidden, n_features, batch_first=True)\n        def forward(self, x):\n            out, _ = self.enc(x)\n            h = out[:, -1, :]\n            z = self.enc_fc(h)\n            dec_in = self.dec_fc(z).unsqueeze(1).repeat(1, x.size(1), 1)\n            out_dec, _ = self.dec(dec_in)\n            return out_dec\n    ae = LSTMAE(X.shape[1])\n    opt = torch.optim.Adam(ae.parameters(), lr=1e-3)\n    loss_fn = nn.MSELoss()\n    # Train briefly\n    ae.train()\n    EPOCHS = 15\n    for epoch in range(EPOCHS):\n        tot = 0\n        for (batch,) in train_loader:\n            opt.zero_grad()\n            rec = ae(batch)\n            loss = loss_fn(rec, batch)\n            loss.backward()\n            opt.step()\n            tot += loss.item()\n        if (epoch+1) % 5 == 0 or epoch==0:\n            print(f'Epoch {epoch+1}/{EPOCHS}, loss={tot/len(train_loader):.6f}')\n    # compute errors across dataset\n    X_all_seq = create_sequences(X_scaled, SEQ)\n    with torch.no_grad():\n        ae.eval()\n        rec_all = ae(torch.tensor(X_all_seq, dtype=torch.float32)).numpy()\n    rec_err = np.mean((rec_all - X_all_seq)**2, axis=(1,2))\n    # map to per-sample\n    err_per_sample = np.zeros(len(X_scaled)); counts = np.zeros(len(X_scaled))\n    for i,e in enumerate(rec_err):\n        err_per_sample[i:i+SEQ] += e\n        counts[i:i+SEQ] += 1\n    err_per_sample /= np.maximum(counts, 1)\n    thr_ae = np.percentile(err_per_sample[clean_mask], 95) if np.any(clean_mask) else np.percentile(err_per_sample, 95)\n    print('AE threshold (95pct on clean):', thr_ae)\nelse:\n    print('Skipping AE section.')", "outputs": []}, {"id": "b83b841c", "cell_type": "markdown", "source": "## Appendix: Using Real Datasets (optional)\n\nIf you want to run these experiments with real-world datasets (e.g., Oxford RobotCar, TEXBAT), follow these steps:\n\n1. Obtain dataset access (Kaggle or direct download).\n2. Preprocess: extract GNSS NMEA logs or receiver CSVs with per-satellite SNR, HDOP, timestamp, and IMU logs.\n3. Replace the synthetic data generator with a loader that produces a DataFrame matching the columns used in this notebook:  \n   `['time','true_lat','true_lon','gnss_lat','gnss_lon','speed','heading','imu_ax','imu_gyro','snr','sat_count','spoofed']`\n4. Re-run feature extraction and model training.\n\n**Legal & safety note:** Do not broadcast spoofed GNSS signals outside a shielded lab. Use recorded traces or GNSS simulators.\n", "metadata": {}}, {"id": "d28e5fce", "cell_type": "markdown", "source": "## Conclusion\n\nThis notebook provides a research-style, reproducible pipeline for exploring GPS spoofing detection.  \nYou can extend it by:\n- Using real GNSS + IMU datasets and richer GNSS metadata (carrier-phase, pseudorange residuals).\n- Implementing a proper EKF/UKF for mitigation.\n- Testing transferability across environments (urban canyon, highway) and adversary types.\n", "metadata": {}}]}