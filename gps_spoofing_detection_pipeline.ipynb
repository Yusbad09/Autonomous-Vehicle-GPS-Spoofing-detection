{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPS Spoofing Detection Pipeline\n",
        "\n",
        "A beginner-friendly, fully-documented notebook implementing:\n",
        "\n",
        "- Synthetic GNSS+IMU dataset generation with injected spoofing\n",
        "- Feature engineering for residual and rolling statistics\n",
        "- Supervised detection using **RandomForest** and **XGBoost** with **Stratified K-Fold CV**\n",
        "- Optional **LSTM Autoencoder** anomaly detector (unsupervised-ish)\n",
        "- **Automated PDF report** generation of results (via reportlab)\n",
        "\n",
        "**How to use this notebook**:\n",
        "1. Run cells **top-to-bottom**.\n",
        "2. The pipeline saves charts and a PDF report under `outputs_cv/`.\n",
        "3. Toggle `DO_AE` to enable/disable the Autoencoder section.\n",
        "\n",
        "> Note: Installing heavy libraries (e.g., xgboost, torch) can take time depending on your environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Environment Setup\n",
        "We install the PDF library used to produce a polished report of metrics and plots. If you're using a managed environment (e.g., Google Colab), this will fetch and install the package as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install reportlab -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Imports\n",
        "Core scientific Python stack, ML models (RandomForest & XGBoost), metrics/utilities, and PyTorch for the optional LSTM Autoencoder. We also import `reportlab` to assemble a PDF report at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ML models\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (confusion_matrix, roc_auc_score, roc_curve,\n",
        "                             precision_recall_fscore_support, accuracy_score)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Deep learning (optional Autoencoder)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# PDF report\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
        "from reportlab.lib.styles import getSampleStyleSheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Configuration\n",
        "Set random seeds for reproducibility, define the output directory, number of CV splits, and whether to run the Autoencoder section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "SAVE_DIR = \"outputs_cv\"   # where outputs + report will be saved\n",
        "N_SPLITS = 5               # K-fold cross validation\n",
        "DO_AE = True               # Run autoencoder anomaly detector?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Utility Helpers\n",
        "Small helper(s) used throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def ensure_dir(path: str) -> None:\n",
        "    \"\"\"Make sure a directory exists.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Synthetic GNSS + IMU Dataset with Spoofing Injection\n",
        "We generate a plausible trajectory and derive displacement, speed, and heading. GNSS observations are corrupted by noise and additionally **spoofed** over specified windows.\n",
        "\n",
        "- *Abrupt spoof*: sudden offsets in latitude/longitude.\n",
        "- *Gradual spoof*: linearly increasing offsets.\n",
        "\n",
        "We also simulate signal quality (SNR) and satellite count, which can carry weak signals of spoofing in some scenarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_synthetic_gnss_imu(n_points=2000,\n",
        "                                spoof_windows=[(400,480),(1100,1180),(1600,1680)],\n",
        "                                abrupt_prob=0.6) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate synthetic GNSS + IMU data with injected spoofing attacks.\n",
        "    \"\"\"\n",
        "\n",
        "    deg_to_m = 111320  # degrees latitude \u2248 meters\n",
        "    # True trajectory\n",
        "    lat = 51.75 + np.cumsum(np.random.normal(0, 0.00002, n_points))\n",
        "    lon = -1.25 + np.cumsum(np.random.normal(0, 0.00002, n_points))\n",
        "\n",
        "    # True displacement\n",
        "    dlat = np.concatenate([[0], np.diff(lat)])\n",
        "    dlon = np.concatenate([[0], np.diff(lon)])\n",
        "    disp_m = np.sqrt((dlat*deg_to_m)**2 + (dlon*deg_to_m*np.cos(np.deg2rad(lat)))**2)\n",
        "    speed = np.clip(disp_m, 0, None)\n",
        "    heading = (np.degrees(np.arctan2(dlon, np.where(dlat==0, 1e-9, dlat))) + 360) % 360\n",
        "\n",
        "    # IMU data\n",
        "    accel = np.concatenate([[0], np.diff(speed)]) + np.random.normal(0,0.02,n_points)\n",
        "    gyro = np.concatenate([[0], np.diff(heading)]) + np.random.normal(0,0.1,n_points)\n",
        "\n",
        "    # GNSS (with noise)\n",
        "    gnss_lat = lat + np.random.normal(0, 1e-5, n_points)\n",
        "    gnss_lon = lon + np.random.normal(0, 1e-5, n_points)\n",
        "\n",
        "    # Spoofing injection\n",
        "    spoofed = np.zeros(n_points, dtype=int)\n",
        "    for (s,e) in spoof_windows:\n",
        "        length = e - s\n",
        "        if np.random.rand() < abrupt_prob:\n",
        "            # Abrupt spoof\n",
        "            gnss_lat[s:e] += np.random.normal(0.0009, 0.0003)\n",
        "            gnss_lon[s:e] += np.random.normal(-0.0009, 0.0003)\n",
        "        else:\n",
        "            # Gradual spoof\n",
        "            gnss_lat[s:e] += np.linspace(0, 0.0012, length) + np.random.normal(0,2e-5,length)\n",
        "            gnss_lon[s:e] += np.linspace(0, -0.0012, length) + np.random.normal(0,2e-5,length)\n",
        "        spoofed[s:e] = 1\n",
        "\n",
        "    # Signal quality\n",
        "    snr = 30 + np.random.normal(0,1,n_points)\n",
        "    snr[spoofed==1] += np.random.normal(2.0,0.8, spoofed.sum())\n",
        "    sat_count = np.random.randint(6,12,n_points)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'time': np.arange(n_points),\n",
        "        'true_lat': lat, 'true_lon': lon,\n",
        "        'gnss_lat': gnss_lat, 'gnss_lon': gnss_lon,\n",
        "        'speed': speed, 'heading': heading,\n",
        "        'imu_ax': accel, 'imu_gyro': gyro,\n",
        "        'snr': snr, 'sat_count': sat_count,\n",
        "        'spoofed': spoofed\n",
        "    })\n",
        "\n",
        "def load_or_generate_dataset():\n",
        "    \"\"\"Load dataset (here we generate synthetic).\"\"\"\n",
        "    return generate_synthetic_gnss_imu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Feature Engineering\n",
        "We compute GNSS displacement, compare it with inertial displacement, and build residuals/rolling statistics. We also include heading change and signal quality metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_features(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute engineered features (displacement, residuals, rolling stats).\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    deg_to_m = 111320\n",
        "\n",
        "    d['dlat_g'] = d['gnss_lat'].diff().fillna(0)\n",
        "    d['dlon_g'] = d['gnss_lon'].diff().fillna(0)\n",
        "    d['dlat_m_g'] = d['dlat_g'] * deg_to_m\n",
        "    d['dlon_m_g'] = d['dlon_g'] * deg_to_m * np.cos(np.deg2rad(d['gnss_lat']))\n",
        "    d['disp_m_g'] = np.sqrt(d['dlat_m_g']**2 + d['dlon_m_g']**2)\n",
        "    d['disp_m_ins'] = d['speed']\n",
        "    d['disp_residual'] = d['disp_m_g'] - d['disp_m_ins']\n",
        "\n",
        "    # Rolling stats\n",
        "    for w in [3,5,11]:\n",
        "        d[f'disp_mean_{w}'] = d['disp_m_g'].rolling(w, min_periods=1).mean()\n",
        "        d[f'disp_std_{w}'] = d['disp_m_g'].rolling(w, min_periods=1).std().fillna(0)\n",
        "\n",
        "    # Heading change\n",
        "    dheading = np.abs(np.diff(np.pad(d['heading'].values,(1,0),'edge')))\n",
        "    d['dheading'] = dheading\n",
        "\n",
        "    features = ['gnss_lat','gnss_lon','speed','heading','disp_m_g','disp_m_ins',\n",
        "                'disp_residual','disp_mean_3','disp_std_3','disp_mean_5','disp_std_5',\n",
        "                'snr','sat_count','dheading']\n",
        "    X = d[features].fillna(0)\n",
        "    y = d['spoofed'].astype(int).values\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Metrics & Plotting Utilities\n",
        "Reusable helpers for computing summary metrics and generating figures saved to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_metrics(y_true,y_pred,y_prob):\n",
        "    \"\"\"Compute accuracy, precision, recall, F1, ROC AUC.\"\"\"\n",
        "    acc = accuracy_score(y_true,y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true,y_pred,average=\"binary\",zero_division=0)\n",
        "    auc = roc_auc_score(y_true,y_prob)\n",
        "    return {\"accuracy\":acc,\"precision\":prec,\"recall\":rec,\"f1\":f1,\"roc_auc\":auc}\n",
        "\n",
        "def plot_confusion(y_true,y_pred,title,path):\n",
        "    cm = confusion_matrix(y_true,y_pred)\n",
        "    fig,ax=plt.subplots(figsize=(3,3))\n",
        "    im=ax.imshow(cm,cmap=\"Blues\")\n",
        "    for (i,j),val in np.ndenumerate(cm):\n",
        "        ax.text(j,i,str(val),ha=\"center\",va=\"center\")\n",
        "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels([\"Clean\",\"Spoof\"]); ax.set_yticklabels([\"Clean\",\"Spoof\"])\n",
        "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\"); ax.set_title(title)\n",
        "    fig.tight_layout(); fig.savefig(path,dpi=150); plt.close(fig)\n",
        "\n",
        "def plot_roc(y_true,y_prob,title,path):\n",
        "    fpr,tpr,_=roc_curve(y_true,y_prob)\n",
        "    auc=roc_auc_score(y_true,y_prob)\n",
        "    fig,ax=plt.subplots(figsize=(4,3))\n",
        "    ax.plot(fpr,tpr,label=f\"AUC={auc:.3f}\")\n",
        "    ax.plot([0,1],[0,1],'--',c=\"gray\")\n",
        "    ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\"); ax.set_title(title); ax.legend()\n",
        "    fig.tight_layout(); fig.savefig(path,dpi=150); plt.close(fig)\n",
        "\n",
        "def plot_metric_bar(metrics_dict,title,path):\n",
        "    fig,ax=plt.subplots(figsize=(5,3))\n",
        "    names=list(metrics_dict.keys()); vals=list(metrics_dict.values())\n",
        "    ax.bar(names,vals)\n",
        "    ax.set_ylim(0,1.05); ax.set_title(title)\n",
        "    for i,v in enumerate(vals):\n",
        "        ax.text(i,v+0.01,f\"{v:.3f}\",ha=\"center\")\n",
        "    fig.tight_layout(); fig.savefig(path,dpi=150); plt.close(fig)\n",
        "\n",
        "def plot_summary_bar(avg_tbl,path):\n",
        "    fig,ax=plt.subplots(figsize=(6,3))\n",
        "    metrics=[\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]\n",
        "    for i,m in enumerate(metrics):\n",
        "        ax.bar([x+i*0.2 for x in range(len(avg_tbl))], avg_tbl[m],width=0.2,label=m)\n",
        "    ax.set_xticks(range(len(avg_tbl))); ax.set_xticklabels(avg_tbl[\"model\"])\n",
        "    ax.legend(); ax.set_ylim(0,1.05); ax.set_title(\"Average Performance Comparison\")\n",
        "    fig.tight_layout(); fig.savefig(path,dpi=150); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) LSTM Autoencoder (Optional)\n",
        "Train an LSTM autoencoder on **clean** samples only. Reconstruction error serves as the anomaly score.\n",
        "\n",
        "- We scale inputs with `StandardScaler`.\n",
        "- The 95th percentile of clean reconstruction error sets the anomaly threshold.\n",
        "- We also compute classification-style metrics by thresholding errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class LSTMAE(nn.Module):\n",
        "    def __init__(self,n_features,emb_size=16):\n",
        "        super().__init__()\n",
        "        self.encoder=nn.LSTM(n_features,emb_size,batch_first=True)\n",
        "        self.decoder=nn.LSTM(emb_size,n_features,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        _,(h,_) = self.encoder(x)\n",
        "        z=h.repeat(x.size(1),1,1).permute(1,0,2)\n",
        "        out,_=self.decoder(z)\n",
        "        return out\n",
        "\n",
        "def train_lstm_autoencoder(X,y,save_dir,epochs=15,bs=32):\n",
        "    \"\"\"Train LSTM autoencoder on clean data only and produce metrics/plots.\"\"\"\n",
        "    scaler=StandardScaler(); Xs=scaler.fit_transform(X)\n",
        "    clean=Xs[y==0]\n",
        "    data=torch.tensor(clean,dtype=torch.float32).unsqueeze(1)\n",
        "    loader=DataLoader(TensorDataset(data,data),batch_size=bs,shuffle=True)\n",
        "\n",
        "    model=LSTMAE(X.shape[1]); opt=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "    lossfn=nn.MSELoss(); losses=[]\n",
        "    for ep in range(1,epochs+1):\n",
        "        tot=0\n",
        "        for xb,_ in loader:\n",
        "            opt.zero_grad(); recon=model(xb); loss=lossfn(recon,xb); loss.backward(); opt.step()\n",
        "            tot+=loss.item()*len(xb)\n",
        "        epoch_loss=tot/len(loader.dataset); losses.append(epoch_loss)\n",
        "        print(f\"[AE] Epoch {ep}/{epochs} | loss={epoch_loss:.6f}\")\n",
        "\n",
        "    # Save loss curve\n",
        "    fig,ax=plt.subplots(); ax.plot(range(1,epochs+1),losses,marker=\"o\")\n",
        "    ax.set_title(\"Autoencoder Training Loss\"); ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss\")\n",
        "    fig.savefig(os.path.join(save_dir,\"ae_loss.png\"),dpi=150); plt.close(fig)\n",
        "\n",
        "    all_data=torch.tensor(scaler.transform(X),dtype=torch.float32).unsqueeze(1)\n",
        "    with torch.no_grad():\n",
        "        recon=model(all_data)\n",
        "        errs=torch.mean((recon-all_data)**2,dim=(1,2)).numpy()\n",
        "    th=np.percentile(errs[y==0],95)\n",
        "    preds=(errs>th).astype(int)\n",
        "\n",
        "    # Metrics for AE\n",
        "    acc=accuracy_score(y,preds)\n",
        "    prec,rec,f1,_=precision_recall_fscore_support(y,preds,average=\"binary\",zero_division=0)\n",
        "    auc=roc_auc_score(y,errs)\n",
        "    metrics={\"accuracy\":acc,\"precision\":prec,\"recall\":rec,\"f1\":f1,\"roc_auc\":auc}\n",
        "\n",
        "    # Error plot\n",
        "    fig,ax=plt.subplots(figsize=(10,3))\n",
        "    ax.plot(errs,label=\"Error\"); ax.axhline(th,color=\"r\",ls=\"--\",label=\"Threshold\")\n",
        "    ax.scatter(np.where(y==1)[0], np.array(errs)[y==1], c=\"orange\",s=10,label=\"Spoof\")\n",
        "    ax.set_title(\"Autoencoder Reconstruction Errors\"); ax.legend()\n",
        "    fig.tight_layout(); fig.savefig(os.path.join(save_dir,\"ae_error_plot.png\"),dpi=150); plt.close(fig)\n",
        "\n",
        "    return {\"model\":model,\"scaler\":scaler,\"errors\":errs,\"threshold\":th,\"losses\":losses,\"metrics\":metrics}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Stratified K-Fold Cross-Validation (RandomForest & XGBoost)\n",
        "We evaluate two supervised classifiers using stratified 5-fold cross-validation and save confusion matrices, ROC curves, and per-fold metric bars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_cross_validation(X,y,out_root,n_splits=5):\n",
        "    \"\"\"Run stratified K-fold cross-validation with RF + XGB.\"\"\"\n",
        "    ensure_dir(out_root)\n",
        "    skf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=RANDOM_SEED)\n",
        "    rows=[]\n",
        "    for fold_idx,(train_idx,test_idx) in enumerate(skf.split(X,y),1):\n",
        "        print(f\"\\n--- Fold {fold_idx}/{n_splits} ---\")\n",
        "        fold_dir=os.path.join(out_root,f\"fold_{fold_idx}\"); ensure_dir(fold_dir)\n",
        "        Xtr,Xte=X.iloc[train_idx],X.iloc[test_idx]; ytr,yte=y[train_idx],y[test_idx]\n",
        "\n",
        "        # RandomForest\n",
        "        rf=RandomForestClassifier(n_estimators=200,random_state=RANDOM_SEED)\n",
        "        rf.fit(Xtr,ytr); ypred=rf.predict(Xte); yprob=rf.predict_proba(Xte)[:,1]\n",
        "        rf_metrics=compute_metrics(yte,ypred,yprob)\n",
        "        print(\"RF:\",rf_metrics)\n",
        "        plot_confusion(yte,ypred,\"RF Confusion\",os.path.join(fold_dir,\"rf_confusion.png\"))\n",
        "        plot_roc(yte,yprob,\"RF ROC\",os.path.join(fold_dir,\"rf_roc.png\"))\n",
        "        plot_metric_bar(rf_metrics,\"RF Metrics\",os.path.join(fold_dir,\"rf_metrics.png\"))\n",
        "        rows.append({\"model\":\"RandomForest\",\"fold\":fold_idx,**rf_metrics})\n",
        "\n",
        "        # XGBoost\n",
        "        xg=xgb.XGBClassifier(n_estimators=200,use_label_encoder=False,eval_metric=\"logloss\",random_state=RANDOM_SEED)\n",
        "        xg.fit(Xtr,ytr); ypred=xg.predict(Xte); yprob=xg.predict_proba(Xte)[:,1]\n",
        "        xg_metrics=compute_metrics(yte,ypred,yprob)\n",
        "        print(\"XGB:\",xg_metrics)\n",
        "        plot_confusion(yte,ypred,\"XGB Confusion\",os.path.join(fold_dir,\"xgb_confusion.png\"))\n",
        "        plot_roc(yte,yprob,\"XGB ROC\",os.path.join(fold_dir,\"xgb_roc.png\"))\n",
        "        plot_metric_bar(xg_metrics,\"XGB Metrics\",os.path.join(fold_dir,\"xgb_metrics.png\"))\n",
        "        rows.append({\"model\":\"XGBoost\",\"fold\":fold_idx,**xg_metrics})\n",
        "\n",
        "    df=pd.DataFrame(rows)\n",
        "    df.to_csv(os.path.join(out_root,\"per_fold_metrics.csv\"),index=False)\n",
        "    avg=df.groupby(\"model\")[['accuracy','precision','recall','f1','roc_auc']].mean().reset_index()\n",
        "    avg.to_csv(os.path.join(out_root,\"average_metrics.csv\"),index=False)\n",
        "    return avg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) PDF Report Builder\n",
        "Creates a single PDF collecting per-fold images, average tables, and autoencoder figures/threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_pdf_report(avg_tbl, save_dir, ae_info=None, n_splits=5):\n",
        "    \"\"\"Build PDF report with tables and plots.\"\"\"\n",
        "    ensure_dir(save_dir)\n",
        "    pdf_path = os.path.join(save_dir, \"report.pdf\")\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elems = []\n",
        "\n",
        "    elems.append(Paragraph(\"GPS Spoofing Detection Report\", styles['Title']))\n",
        "    elems.append(Paragraph(f\"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}\", styles['Normal']))\n",
        "    elems.append(Spacer(1, 12))\n",
        "\n",
        "    # Per-fold metrics table\n",
        "    per_fold_csv = os.path.join(save_dir, \"per_fold_metrics.csv\")\n",
        "    if os.path.exists(per_fold_csv):\n",
        "        per_fold_df = pd.read_csv(per_fold_csv)\n",
        "        elems.append(Paragraph(\"Per-Fold Metrics:\", styles['Heading2']))\n",
        "        elems.append(Paragraph(per_fold_df.to_html(index=False), styles['Normal']))\n",
        "        elems.append(Spacer(1, 12))\n",
        "\n",
        "    # Average metrics\n",
        "    elems.append(Paragraph(\"Average Metrics (Cross Validation):\", styles['Heading2']))\n",
        "    elems.append(Paragraph(avg_tbl.to_html(index=False), styles['Normal']))\n",
        "    elems.append(Spacer(1, 12))\n",
        "\n",
        "    # Per-fold plots\n",
        "    for fold_idx in range(1, n_splits+1):\n",
        "        fold_dir = os.path.join(save_dir, f\"fold_{fold_idx}\")\n",
        "        if not os.path.exists(fold_dir):\n",
        "            continue\n",
        "        elems.append(Paragraph(f\"Results for Fold {fold_idx}:\", styles['Heading2']))\n",
        "        for model in [\"rf\",\"xgb\"]:\n",
        "            for plot in [\"confusion\",\"roc\",\"metrics\"]:\n",
        "                img_path = os.path.join(fold_dir, f\"{model}_{plot}.png\")\n",
        "                if os.path.exists(img_path):\n",
        "                    elems.append(Image(img_path, width=400, height=250))\n",
        "                    elems.append(Spacer(1, 12))\n",
        "\n",
        "    # Autoencoder section\n",
        "    if ae_info:\n",
        "        elems.append(Paragraph(\"LSTM Autoencoder Results:\", styles['Heading2']))\n",
        "        metrics_df=pd.DataFrame([ae_info['metrics']]) if 'metrics' in ae_info else None\n",
        "        if metrics_df is not None:\n",
        "            elems.append(Paragraph(metrics_df.to_html(index=False), styles['Normal']))\n",
        "            elems.append(Spacer(1, 10))\n",
        "        elems.append(Paragraph(f\"95th percentile threshold: {ae_info['threshold']:.4f}\", styles['Normal']))\n",
        "        loss_plot = os.path.join(save_dir,\"ae_loss.png\")\n",
        "        ae_plot = os.path.join(save_dir,\"ae_error_plot.png\")\n",
        "        if os.path.exists(loss_plot):\n",
        "            elems.append(Image(loss_plot, width=400, height=250))\n",
        "        if os.path.exists(ae_plot):\n",
        "            elems.append(Image(ae_plot, width=400, height=150))\n",
        "\n",
        "    doc.build(elems)\n",
        "    print(f\"\\n\u2705 Full PDF report will be saved at: {pdf_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Main: Run Everything\n",
        "Execute this cell to:\n",
        "1. Generate the dataset and features\n",
        "2. Run CV for RandomForest & XGBoost (and save plots)\n",
        "3. Optionally train the AE and generate its plots/metrics\n",
        "4. Build the consolidated PDF report in `outputs_cv/report.pdf`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def main():\n",
        "    ensure_dir(SAVE_DIR)\n",
        "    df = load_or_generate_dataset()\n",
        "    print(\"Dataset shape:\", df.shape)\n",
        "    X, y = extract_features(df)\n",
        "    print(\"Feature matrix:\", X.shape, \"| Labels:\", y.shape)\n",
        "\n",
        "    # Cross-validation and plots\n",
        "    results = run_cross_validation(X, y, SAVE_DIR, N_SPLITS)\n",
        "    print(\"\\nAverage CV metrics:\\n\", results)\n",
        "\n",
        "    # Optional Autoencoder\n",
        "    ae_info = None\n",
        "    if DO_AE:\n",
        "        ae_info = train_lstm_autoencoder(X, y, SAVE_DIR, epochs=15, bs=32)\n",
        "        print(\"\\nAE metrics:\\n\", ae_info.get('metrics', {}))\n",
        "\n",
        "    # PDF report\n",
        "    generate_pdf_report(results, SAVE_DIR, ae_info, N_SPLITS)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Appendix: Notes & Tips\n",
        "- **Data realism**: The dataset is synthetic and intended for didactic purposes\u2014expect different behavior on real logs.\n",
        "- **Hyperparameters**: Feel free to tune `n_estimators`, learning rates, or add regularization to XGBoost.\n",
        "- **Scaling**: Tree-based models generally don't need scaling; the AE does\u2014handled internally.\n",
        "- **Compute**: Autoencoders may benefit from a GPU; CPU works but can be slower.\n",
        "- **Extensions**:\n",
        "  - Try additional features (e.g., Doppler residuals, CN0 stats per-satellite).\n",
        "  - Incorporate temporal windows (sequence models for the supervised side).\n",
        "  - Calibrate thresholds using validation curves.\n"
      ]
    }
  ]
}